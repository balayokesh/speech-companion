{"version":3,"sources":["Images/microphone.svg","Images/microphone-muted.svg","Images/speaker-muted.svg","Images/speaker.svg","App.js","index.js"],"names":["App","useSpeechRecognition","transcript","listening","browserSupportsSpeechRecognition","id","src","Microphone","MicrophoneMuted","alt","className","onClick","SpeechRecognition","stopListening","startListening","contentEditable","window","location","reload","SpeakerMuted","document","getElementById","Speaker","msg","SpeechSynthesisUtterance","text","textContent","speechSynthesis","speak","addEventListener","ReactDOM","render","StrictMode"],"mappings":"0OAAe,MAA0B,uCCA1B,MAA0B,6CCA1B,MAA0B,0CCA1B,MAA0B,oC,OCwD1BA,MA5Cf,WAaE,MAIIC,iCAHFC,EADF,EACEA,WACAC,EAFF,EAEEA,UAQF,OAVA,EAGEC,iCAYA,sBAAKC,GAAG,cAAR,UACI,qBAAKC,IAAKH,EAAYI,EAAaC,EAAiBH,GAAG,aAAaI,IAAI,aAAaC,UAAU,eAAeC,QAASR,EAAYS,IAAkBC,cAAgBD,IAAkBE,iBACvL,sBAAKJ,UAAU,eAAeL,GAAG,SAAjC,UACE,qBAAKU,iBAAe,EAACV,GAAG,WAAxB,SACGH,EAAa,4BAAIA,IAAkB,+IAItC,wBAAQS,QAjBF,WACZK,OAAOC,SAASC,UAgBcb,GAAG,QAA3B,sBAGF,qBAAKC,IAAKa,EAAcR,QArC9B,WACES,SAASC,eAAe,WAAWf,IAAMgB,EACzC,IAAIC,EAAM,IAAIC,yBACdD,EAAIE,KAAOL,SAASC,eAAe,YAAYK,YAC/CV,OAAOW,gBAAgBC,MAAML,GAE7BA,EAAIM,iBAAiB,OAAO,WAC1BT,SAASC,eAAe,WAAWf,IAAMa,MA8BQd,GAAG,UAAUI,IAAI,UAAUC,UAAU,oBAfjF,gFC/BXoB,IAASC,OACP,cAAC,IAAMC,WAAP,UACE,cAAC,EAAD,MAEFZ,SAASC,eAAe,W","file":"static/js/main.d3cfb86f.chunk.js","sourcesContent":["export default __webpack_public_path__ + \"static/media/microphone.b9764a69.svg\";","export default __webpack_public_path__ + \"static/media/microphone-muted.bcadbc7e.svg\";","export default __webpack_public_path__ + \"static/media/speaker-muted.0347a829.svg\";","export default __webpack_public_path__ + \"static/media/speaker.6a264f35.svg\";","// Import style sheets\r\nimport './Styles/App.css'; \r\n\r\n// Import NPM packages\r\nimport SpeechRecognition, { useSpeechRecognition } from 'react-speech-recognition'\r\n\r\n// Import images\r\nimport Microphone from './Images/microphone.svg';\r\nimport MicrophoneMuted from './Images/microphone-muted.svg';\r\nimport SpeakerMuted from './Images/speaker-muted.svg';\r\nimport Speaker from './Images/speaker.svg';\r\n\r\nfunction App() {\r\n\r\n  function speakContent() {\r\n    document.getElementById(\"speaker\").src = Speaker;\r\n    var msg = new SpeechSynthesisUtterance();\r\n    msg.text = document.getElementById('textarea').textContent;\r\n    window.speechSynthesis.speak(msg);\r\n    \r\n    msg.addEventListener(\"end\", () => {\r\n      document.getElementById(\"speaker\").src = SpeakerMuted;\r\n    });\r\n  }\r\n\r\n  const {\r\n    transcript,\r\n    listening,\r\n    browserSupportsSpeechRecognition\r\n  } = useSpeechRecognition();  \r\n\r\n  const reset = () => {\r\n    window.location.reload();\r\n  }\r\n    \r\n  if (!browserSupportsSpeechRecognition) {\r\n    return <span>Browser doesn't support speech recognition.</span>;\r\n  }\r\n\r\n  return (\r\n    <div id='flex-parent'>\r\n        <img src={listening ? Microphone : MicrophoneMuted} id='microphone' alt='microphone' className='flex-element' onClick={listening ? SpeechRecognition.stopListening : SpeechRecognition.startListening }/>\r\n        <div className='flex-element' id='center'>\r\n          <div contentEditable id='textarea'>\r\n            {transcript ? <p>{transcript}</p> : <p>Click the mike button and start speaking\r\n            (or)\r\n            Click on right speaker button to speak the contents in the box</p>}\r\n          </div>\r\n          <button onClick={reset} id='reset'>Reset</button>\r\n        </div>\r\n        \r\n        <img src={SpeakerMuted} onClick={speakContent} id='speaker' alt='speaker' className='flex-element'/>\r\n    </div>\r\n  );\r\n}\r\n\r\nexport default App;\r\n","import React from 'react';\r\nimport ReactDOM from 'react-dom';\r\nimport './Styles/index.css';\r\nimport App from './App';\r\n\r\nReactDOM.render(\r\n  <React.StrictMode>\r\n    <App />\r\n  </React.StrictMode>,\r\n  document.getElementById('root')\r\n);\r\n"],"sourceRoot":""}