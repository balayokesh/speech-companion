{"version":3,"sources":["Images/microphone.svg","Images/microphone-muted.svg","Images/speaker-muted.svg","App.js","index.js"],"names":["App","useSpeechRecognition","transcript","listening","browserSupportsSpeechRecognition","id","src","Microphone","MicrophoneMuted","alt","className","onClick","SpeechRecognition","stopListening","startListening","contentEditable","window","location","reload","SpeakerMuted","msg","SpeechSynthesisUtterance","text","document","getElementById","innerHTML","speechSynthesis","speak","ReactDOM","render","StrictMode"],"mappings":"0OAAe,MAA0B,uCCA1B,MAA0B,6CCA1B,MAA0B,0C,OCkD1BA,MAvCf,WAEE,IAMA,EAIIC,iCAHFC,EADF,EACEA,WACAC,EAFF,EAEEA,UAQF,OAVA,EAGEC,iCAYA,sBAAKC,GAAG,cAAR,UACI,qBAAKC,IAAKH,EAAYI,EAAaC,EAAiBH,GAAG,aAAaI,IAAI,aAAaC,UAAU,eAAeC,QAASR,EAAYS,IAAkBC,cAAgBD,IAAkBE,iBACvL,sBAAKJ,UAAU,eAAeL,GAAG,SAAjC,UACE,qBAAKU,iBAAe,EAACV,GAAG,WAAxB,SACGH,EAAa,4BAAIA,IAAkB,+IAItC,wBAAQS,QAjBF,WACZK,OAAOC,SAASC,UAgBcb,GAAG,QAA3B,sBAGF,qBAAKC,IAAKa,EAAcR,QAhCT,WACnB,IAAIS,EAAM,IAAIC,yBACdD,EAAIE,KAAOC,SAASC,eAAe,YAAYC,UAC/CT,OAAOU,gBAAgBC,MAAMP,IA6BsBf,GAAG,UAAUI,IAAI,UAAUC,UAAU,oBAfjF,gFCzBXkB,IAASC,OACP,cAAC,IAAMC,WAAP,UACE,cAAC,EAAD,MAEFP,SAASC,eAAe,W","file":"static/js/main.fa407e91.chunk.js","sourcesContent":["export default __webpack_public_path__ + \"static/media/microphone.b9764a69.svg\";","export default __webpack_public_path__ + \"static/media/microphone-muted.bcadbc7e.svg\";","export default __webpack_public_path__ + \"static/media/speaker-muted.0347a829.svg\";","// Import style sheets\nimport './Styles/App.css'; \n\n// Import NPM packages\nimport SpeechRecognition, { useSpeechRecognition } from 'react-speech-recognition'\n\n// Import images\nimport Microphone from './Images/microphone.svg';\nimport MicrophoneMuted from './Images/microphone-muted.svg';\nimport SpeakerMuted from './Images/speaker-muted.svg';\n\nfunction App() {\n\n  const speakContent = () => {\n    var msg = new SpeechSynthesisUtterance();\n    msg.text = document.getElementById('textarea').innerHTML;\n    window.speechSynthesis.speak(msg);\n  }\n\n  const {\n    transcript,\n    listening,\n    browserSupportsSpeechRecognition\n  } = useSpeechRecognition();  \n\n  const reset = () => {\n    window.location.reload();\n  }\n    \n  if (!browserSupportsSpeechRecognition) {\n    return <span>Browser doesn't support speech recognition.</span>;\n  }\n\n  return (\n    <div id='flex-parent'>\n        <img src={listening ? Microphone : MicrophoneMuted} id='microphone' alt='microphone' className='flex-element' onClick={listening ? SpeechRecognition.stopListening : SpeechRecognition.startListening }/>\n        <div className='flex-element' id='center'>\n          <div contentEditable id='textarea'>\n            {transcript ? <p>{transcript}</p> : <p>Click the mike button and start speaking\n            (or)\n            Click on right speaker button to speak the contents in the box</p>}\n          </div>\n          <button onClick={reset} id='reset'>Reset</button>\n        </div>\n        \n        <img src={SpeakerMuted} onClick={speakContent} id='speaker' alt='speaker' className='flex-element'/>\n    </div>\n  );\n}\n\nexport default App;\n","import React from 'react';\nimport ReactDOM from 'react-dom';\nimport './Styles/index.css';\nimport App from './App';\n\nReactDOM.render(\n  <React.StrictMode>\n    <App />\n  </React.StrictMode>,\n  document.getElementById('root')\n);\n"],"sourceRoot":""}